# RAG Implementation Setup Guide

## üéØ What Was Fixed

Your chatbot was **creating embeddings but NOT using them** for retrieval. It was doing basic keyword matching instead of semantic vector search. This caused:
- ‚ùå Generic, data-oriented responses
- ‚ùå Chatbot not understanding knowledge base context
- ‚ùå Custom instructions being ignored

**Now it uses proper RAG (Retrieval-Augmented Generation) with:**
- ‚úÖ Vector embeddings via OpenRouter
- ‚úÖ Semantic similarity search
- ‚úÖ Precise knowledge-based responses

---

## üì¶ Changes Made

### 1. **OpenRouter Client** (`src/lib/openrouter.ts`)
- Added `generateEmbedding()` method
- Uses `openai/text-embedding-3-small` model (768 dimensions)
- Handles text truncation and proper error handling

### 2. **Chat API** (`src/api/chat.ts`)
- Replaced keyword matching with vector similarity search
- Generates embeddings for user queries
- Uses Supabase RPC function for vector search
- Fallback to keyword search if vector search fails

### 3. **Supabase Migration** (`supabase/migrations/create_vector_search.sql`)
- Updated to 768 dimensions (was 1536)
- Fixed `match_knowledge_chunks()` function signature
- Accepts array of source IDs instead of single agent ID
- Proper vector indexing with IVFFlat

### 4. **Edge Function** (`supabase/functions/process-knowledge/index.ts`)
- Fixed model name: `openai/text-embedding-3-small`
- Added proper headers (HTTP-Referer, X-Title)
- Better error handling and validation

---

## üöÄ Setup Instructions

### Step 1: Run Supabase Migration

Run this SQL in your Supabase SQL Editor:

```sql
-- Enable pgvector extension
create extension if not exists vector;

-- Add/update embedding column to 768 dimensions
alter table knowledge_chunks 
drop column if exists embedding;

alter table knowledge_chunks 
add column embedding vector(768);

-- Create index for fast vector search
drop index if exists knowledge_chunks_embedding_idx;
create index knowledge_chunks_embedding_idx 
on knowledge_chunks using ivfflat (embedding vector_cosine_ops)
with (lists = 100);

-- Update function for vector similarity search
create or replace function match_knowledge_chunks(
  query_embedding vector(768),
  agent_sources uuid[],
  match_threshold float default 0.5,
  match_count int default 3
)
returns table (
  id uuid,
  source_id uuid,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
begin
  return query
  select
    kc.id,
    kc.source_id,
    kc.content,
    kc.metadata,
    1 - (kc.embedding <=> query_embedding) as similarity
  from knowledge_chunks kc
  where 
    kc.source_id = any(agent_sources)
    and kc.embedding is not null
    and 1 - (kc.embedding <=> query_embedding) > match_threshold
  order by kc.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Grant permissions
grant execute on function match_knowledge_chunks to authenticated, anon;
```

### Step 2: Set OpenRouter API Key

Make sure your `.env` file has:

```env
VITE_OPENROUTER_API_KEY=your_openrouter_key_here
```

Also add it to your Supabase Edge Functions environment variables:

1. Go to Supabase Dashboard ‚Üí Project Settings ‚Üí Edge Functions
2. Add environment variable:
   - Name: `OPENROUTER_API_KEY`
   - Value: `your_openrouter_key_here`

### Step 3: Deploy Edge Function (if needed)

If you're using Supabase locally:

```bash
supabase functions deploy process-knowledge
```

### Step 4: Re-process All Existing Knowledge

**IMPORTANT**: Existing knowledge sources need to be re-processed to generate embeddings with the new 768-dimension format.

**Option A: Delete and Re-add Knowledge** (Recommended)
1. Go to Knowledge Manager
2. Delete all existing knowledge sources
3. Re-add them (they'll be processed with new embeddings)

**Option B: Trigger Re-processing via API**

For each knowledge source, call:

```bash
curl -X POST https://your-project.supabase.co/functions/v1/process-knowledge \
  -H "Authorization: Bearer YOUR_ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{"sourceId": "source-uuid-here"}'
```

---

## üß™ Testing

### 1. Test Embeddings Generation

```typescript
import { openRouterClient } from './src/lib/openrouter';

const embedding = await openRouterClient.generateEmbedding("test query");
console.log('Embedding dimensions:', embedding.length); // Should be 768
```

### 2. Test Vector Search

After adding knowledge, check logs when chatting:

```
[RAG] Searching in X knowledge sources
[RAG] Query embedding generated (768 dimensions)
[RAG] Found 3 relevant chunks via vector search
```

### 3. Test Chat Responses

Ask specific questions from your knowledge base:
- ‚úÖ Should return precise answers from knowledge
- ‚úÖ Should cite sources
- ‚úÖ Should NOT give generic AI responses
- ‚úÖ Should say "outside my knowledge base" if not found

---

## üìä How It Works Now

### Before (Broken):
```
User Query ‚Üí Keyword Matching ‚Üí Dump Entire Knowledge Base ‚Üí Generic AI Response ‚ùå
```

### After (Fixed):
```
User Query ‚Üí Generate Embedding (768-dim vector)
           ‚Üì
         Vector Search (cosine similarity)
           ‚Üì
         Top 3 Relevant Chunks (>0.5 similarity)
           ‚Üì
         Inject into System Prompt
           ‚Üì
         AI Response ONLY from Context ‚úÖ
```

---

## üîç Debugging

### Check if embeddings exist:

```sql
SELECT 
  ks.name,
  COUNT(kc.id) as total_chunks,
  COUNT(kc.embedding) as chunks_with_embeddings
FROM knowledge_sources ks
LEFT JOIN knowledge_chunks kc ON kc.source_id = ks.id
GROUP BY ks.id, ks.name;
```

### Check vector search function:

```sql
-- Test with dummy vector
SELECT match_knowledge_chunks(
  array_fill(0, ARRAY[768])::vector(768),
  ARRAY['your-source-id']::uuid[],
  0.3,
  5
);
```

### Enable detailed logs:

In `src/api/chat.ts`, all RAG operations log with `[RAG]` prefix. Check console for:
- Query embedding generation
- Vector search results
- Similarity scores
- Fallback triggers

---

## üéõÔ∏è Configuration

### Adjust Similarity Threshold

In `src/api/chat.ts` line ~170:

```typescript
match_threshold: 0.5,  // Lower = more results (0.3-0.7 recommended)
match_count: 3,        // Number of chunks to retrieve
```

### Adjust Chunk Size

In `supabase/functions/process-knowledge/index.ts` line ~208:

```typescript
const chunks = chunkText(source.content, 800, 150)
//                                      ^^^  ^^^ overlap tokens
//                                      target tokens per chunk
```

---

## ‚úÖ Success Indicators

1. **Embeddings Generated**: Check `knowledge_chunks` table has non-null `embedding` column
2. **Vector Search Working**: Logs show `[RAG] Found X relevant chunks via vector search`
3. **Precise Responses**: Chatbot answers from knowledge, not generic responses
4. **Source Citation**: Responses include `[Source 1]` references

---

## üêõ Common Issues

### Issue: "Function match_knowledge_chunks does not exist"
**Fix**: Run the migration SQL again

### Issue: "column embedding does not exist"
**Fix**: Run the `alter table` commands from Step 1

### Issue: "Embedding generation failed"
**Fix**: Check OpenRouter API key is set in both `.env` and Supabase Edge Functions

### Issue: Still getting generic responses
**Fix**: 
1. Delete old knowledge and re-add
2. Check console for `[RAG]` logs
3. Verify embeddings exist in database

---

## üìà Performance

- **Embedding Model**: `openai/text-embedding-3-small`
- **Dimensions**: 768
- **Cost**: ~$0.00002 per 1K tokens (very cheap)
- **Search Speed**: <100ms with IVFFlat index
- **Chunk Size**: ~800 tokens with 150 token overlap

---

## üéì Understanding the Code

### Vector Similarity
```
similarity = 1 - cosine_distance
```
- 1.0 = identical
- 0.5 = threshold (configurable)
- 0.0 = completely different

### Why 768 dimensions?
- `text-embedding-3-small` outputs 768-dim vectors
- Good balance of quality vs. performance
- 10x smaller than 1536-dim models

### Why chunk overlap?
- Maintains context at chunk boundaries
- 150 token overlap ensures no information loss
- Critical for questions spanning multiple paragraphs

---

Need help? Check the console logs with `[RAG]` prefix for detailed debugging info.
